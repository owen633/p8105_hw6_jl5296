---
title: "HW 6"
author: "Jianyou Liu"
date: "November 25, 2018"
output: github_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(modelr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

```

## Problem 1
### Homicide Data in Major US Cities

####Load in dataset

```{r import_raw_data, message=FALSE}
raw_hom_data = read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")

head(raw_hom_data)

```

#### Create city_state variable and binary variable indicating whether the homicide is solved

```{r mutate_data}
new_hom_data1 = raw_hom_data %>% 
  unite(city, state, col = "city_state", sep = ",") %>% 
  mutate(bin_solved = ifelse(disposition == "Closed without arrest" | disposition == "Open/No arrest", 0, 1))

```
**Note**: '0' = crime unsolved; '1' = crime solved

#### Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake
```{r filter_data}
new_hom_data2 = new_hom_data1 %>% 
  filter(city_state != "Dallas,TX", city_state != "Phoenix,AZ", city_state != "Kansas City,MO", city_state != "Tulsa,AL")

```
#### Modifiy victim_race to have categories white and non-white, with white as the reference category. Make sure that victim_age is a numeric variable.

```{r manipuate_data}
final_hom_data = new_hom_data2 %>% 
  mutate(victim_race = ifelse(victim_race == "White", "White", "Non-White"),
  # Make 'White' as the reference category
  victim_race = fct_relevel(victim_race, ref = "White")) %>% 
  # Convert 'victim_age' into numeric
  mutate(victim_age = as.numeric(victim_age))

head(final_hom_data, n = 10)

```
### Baltimore, MD Linear Model Analysis
#### Fit logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors.

```{r baltimore_glm}
balt_data = final_hom_data %>% 
  filter(city_state == "Baltimore,MD")

# Save output as R object
fit_glm_balt = balt_data %>% 
  glm(bin_solved ~ victim_age + victim_sex + victim_race, data = ., family = binomial())

```
#### Apply broom::tidy to the object and extract estimate and confidence interval of the adjusted odds ratio

```{r baltimore_est_ci}
fit_glm_balt %>% 
  broom::tidy() %>% 
  # Transform log 'Odds Ratio' back; calculate 95% CIs
  mutate(OR = exp(estimate), low_CI = exp(estimate - std.error*1.96), high_CI = exp(estimate + std.error*1.96)) %>% 
  select(term, OR, low_CI, high_CI, p.value) %>% 
  # Compare 'non-white' to 'white victims'
  filter(term == "victim_raceNon-White") %>% 
  knitr::kable(digits = 3)

```


**Interpretation**: We can be 95% confident that the odds of resolving a homicide case is between 0.322 and 0.637 times among *non-white* victims than that of *white* victims.

### All City Analysis
#### Run logistic regression for each of the cities; extract the adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims

```{r glm_all}
fit_glm_all = final_hom_data %>% 
  group_by(city_state) %>% 
  # Generate list column
  nest() %>% 
  # Perform glm and broom;tidy for each city
  mutate(logis_model = map(data, ~glm(bin_solved ~ victim_age + victim_sex + victim_race, data = ., family = binomial())), logis_model = map(logis_model, broom::tidy)) %>% 
  select(-data) %>% 
  unnest() %>% 
  # Obtain odds ratio and compute 95% confidence intervals
  mutate(OR = exp(estimate), low_CI = exp(estimate - std.error*1.96), high_CI = exp(estimate + std.error*1.96)) %>% 
  select(city_state, term, OR, low_CI, high_CI, p.value) %>% 
  filter(term == "victim_raceNon-White")

# Display first 10 rows of resulting data frame
head(fit_glm_all, n = 10) %>% 
  knitr::kable(digits = 3)
  
```

#### Create a plot that shows the estimated ORs and CIs for each city

```{r plot_all_city}
fit_glm_all %>% 
  # Organize cities according to increasing estimated ORs
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point(alpha = .5) +
  geom_errorbar(aes(ymin = low_CI , ymax = high_CI), color = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Estimates of OR and CIs for each City for Whites vs. Non - Whites",
    x = "City",
    y = "Estimated Adjusted OR"
  )

```
**Comments**: Looking at the estimated ORs, Boston has the least value suggesting that cases in this city involving non-white victims have the lowest chance of being solved than those involving white residents among the all cities in the dataset. Furthermore, the majority of the cities have ORs less than 1, indicating the possibility that homicides done to non-white subjects are at lower odds of being resolved compared with those done to white people in most places. There are 3 cities in which the adjusted ORs are above 1, Birmingham, Durham, Tampa, meaning that these areas probably have opposite phenomenons, that crimes with white victims have less likelihood of being solved.

However, we cannot be entirely sure that the findings are true since by examining the constructed condifence intervals, the intervals for some cities do include the null hypothesized value 1, which signals no association. For these places, we need to investigage their individual p-values to determine whether the estimates are statistically significant or not.


## Problem 2
### Variables that affect Child's Birthweight

#### Load and Clean Dataset
```{r read_clean_data, message=FALSE}
raw_birth_data = read_csv("./data/birthweight.csv")

# Check for missing values
raw_birth_data[!complete.cases(raw_birth_data),]

# Convert numeric to factor where appropriate
tidy_birth_data = raw_birth_data %>% 
  mutate(babysex = as.factor(babysex), frace = as.factor(frace), mrace = as.factor(mrace), malform = as.factor(malform))

head(tidy_birth_data, n = 10)
  

```
**Note**: The line of code checking for NAs outputs 0 rows, which indicates that the data does not contain missing values.

#### Propose regression model for birthweight
```{r backward_eli, eval=FALSE}
# Backward elimination
fit_all = lm(bwt ~ ., data = tidy_birth_data)
summary(fit_all)
step1 = update(fit_all, . ~ . -malform)
summary(step1)
step2 = update(step1, . ~ . -ppbmi)
summary(step2)
step3 = update(step2, . ~ . -frace)
summary(step3)

# Repeat process a number of times to select desirable model
```
The modeling process I used is backwards elimimation, which first fits a multiple linear regression model for all variables, then removing the non-significant ones(p-value > 0.05) one at a time starting with the highest one. Eventually, the predictors that I chose to keep in my model are 'babysex', 'head circumference', 'length', 'mother's weight', 'gestational age', 'mother's height', 'mother's race', 'parity', 'mother's pre-pregnancy weight', and 'smoking status'.

#### Show a plot of model residuals against fitted values
```{r resid_fit_plot}
# Final linear model selected
prop_fit_lm = lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + mheight + mrace + parity + ppwt + smoken, data = tidy_birth_data)

summary(prop_fit_lm)

# Construct plot of residuals vs. fitted values
tidy_birth_data %>% 
  add_predictions(prop_fit_lm) %>% 
  add_residuals(prop_fit_lm) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .5) +
  labs (
    title = "Rediduals vs. Fitted Values",
     x = "Fitted Values",
     y = "Residuals" )
  

```

From the plot, most of the residual values are scattered around '0' with no apparent pattern, which indicates a good chance of equal variance. However, there are some unusual points that deviates from the horizontal line across '0' suggesting the possibility of potential outliers.

### Compare model with 2 others
#### Build the 2 models and display coefficients for predictors
```{r models}
# Construct the other 2 models as directed
comp_lm_2 = lm(bwt ~ blength + gaweeks, data = tidy_birth_data)
comp_lm_3 = lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = tidy_birth_data)

summary(comp_lm_2)
summary(comp_lm_3)
 
```
#### Cross Validation for Prediction Error
```{r cv}
# Perform crossv_mc function
cv_df = crossv_mc(tidy_birth_data, 100) %>% 
  mutate(train = map(train, as_tibble), test = map(test, as_tibble))

# Fit models to training data and obtain corresponding RMSEs for testing data
cv_df = cv_df %>% 
  mutate(prop_fit_lm = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)), 
         comp_lm_2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)), 
         comp_lm_3 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = .x))) %>% 
  mutate(rmse_self_chosen = map2_dbl(prop_fit_lm, test, ~rmse(model = .x, data = .y)),
         rmse_model_2 = map2_dbl(comp_lm_2, test, ~rmse(model = .x, data = .y)),
         rmse_model_3 = map2_dbl(comp_lm_3, test, ~rmse(model = .x, data = .y)))
  
```
#### Make violin plot to visually compare the models
```{r violin_plot}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""), 
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse))+
  geom_violin()
```
**Comment**: Based on the plot, since my own proposed model has the lowest rmse value among the 3 candidates, I would pick this one as the best fit.


